#  EarthMarker:  A Visual Prompting Multi-modal Large Language Model for Remote Sensing

Official repository for [EarthMarker](https://ieeexplore.ieee.org/document/10817639). 

Authors: Wei Zhang*, Miaoxin Cai*, Tong Zhang, Yin Zhuang, and Xuerui Mao
* The authors contributed equally to this work.


  
## :mega: News
- [2024.01.02]: The entire dataset is being uploaded, expected to be completed within a week! :fire:
- [2024.12.22]: EarthMarker has been accepted to [IEEE TGRS](https://ieeexplore.ieee.org/document/10817639). ðŸŽ‰ 
* [2024.07.19]: The paper for EarthMarker is released [arxiv](https://arxiv.org/abs/2407.13596). 


##  :sparkles: Introduction
A visual prompting MLLM called EarthMarker is proposed in the RS domain for the first time. EarthMarker can comprehend remote sensing (RS) imagery under visual and text joint prompts, and flexibly switch interpretation levels, including image, region, and point levels. More importantly, the proposed EarthMarker fills the gap in visual prompting MLLMs for RS, significantly catering to the fine-grained interpretation needs of RS imagery in real-world applications. EarthMarker is capable of various RS visual tasks including scene classification, referring object classification, captioning, and relationship analyses, which are beneficial to making informed decisions in real-world applications.
 <div align="center">
  <img src="VP-example.png">
</div>

##  :sparkles: Dataset is being uploaded!



## :bookmark: Citation
```bash
@article{zhang2024earthmarker,
  title={EarthMarker: A Visual Prompting Multi-modal Large Language Model for Remote Sensing},
  author={Zhang, Wei and Cai, Miaoxin and Zhang, Tong and Zhuang, Yin and Li, Jun and Mao, Xuerui},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  year={2024},
  publisher={IEEE}
}
```

## :memo: Acknowledgment
This paper benefits from [llama](https://github.com/facebookresearch/llama). Thanks for their wonderful work.



